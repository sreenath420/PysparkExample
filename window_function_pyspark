i/p

+----------+---------+------+
|first_name|last_name|salary|
+----------+---------+------+
|      John|    Smith| 20000|
|       Ava|   Ninson| 10000|
|    Cailin|    Smith| 30000|
|      Mike| Peterson| 20000|
|       Ian| Peterson| 80000|
|      John|    Mills| 50000|
+----------+---------+------+

o/p-

+----------+---------+------+---+
|first_name|last_name|salary| sa|
+----------+---------+------+---+
|      John|    Smith| 20000|  2|
|      Mike| Peterson| 20000|  2|
+----------+---------+------+---+


from pyspark.sql.types import StructType,StructField, StringType, IntegerType
data=[("John","Smith",20000),
      ("Ava","Ninson",10000),
	  ("Cailin","Smith",30000),
	  ("Mike","Peterson",20000),
   ("Ian","Peterson",80000),
   ("John","Mills",50000)]
   
schema = StructType([
       StructField("first_name",StringType(),True), \
	   StructField("last_name",StringType(),True), \
	   StructField("salary",IntegerType(),True)])

df = spark.createDataFrame(data=data,schema=schema)
df.printSchema()       
solution-
from pyspark.sql.window import Window
from pyspark.sql.functions import dense_rank
window=Window.orderBy("salary")
window_df=df.withColumn("sa",dense_rank().over(window))
df_final=window_df.filter("sa=2").show()

---------------------------->Write a solution to report all the employees with
their primary department. For employees who belong
to one department, report their only department
Employees can belong to multiple departments. When
the employee joins other departments, they need to
decide which department is their primary department.
Note that when an employee belongs to only one
department, their primary column is 'N'.<-------------------------------------


input

+-------------+-------------+------------+
|employment_id|department_id|primary_flag|
+-------------+-------------+------------+
|            1|            1|           N|
|            2|            1|           Y|
|            2|            2|           N|
|            3|            3|           N|
|            4|            2|           N|
|            4|            3|           Y|
|            4|            4|           N|
+-------------+-------------+------------+

output

+-------------+-------------+
|employment_id|department_id|
+-------------+-------------+
|            1|            1|
|            2|            1|
|            3|            3|
|            4|            3|
+-------------+-------------+

from pyspark.sql.types import StructType,StructField, StringType, IntegerType

data=[
(1,1,'N'),
(2,1,'Y'),
(2,2,'N'),
(3,3,'N'),
(4,2,'N'),
(4,3,'Y'),
(4,4,'N')]
schema = StructType([
StructField ("employment_id", IntegerType(), True),\
StructField ("department_id", IntegerType(), True),\
StructField ("primary_flag", StringType(), True),\
])

df = spark.createDataFrame(data=data,schema=schema)

sql way

df.createOrReplaceTempView('department')

df2=spark.sql("""with cte as(
              select employment_id,department_id,primary_flag,dense_rank() over (partition by employment_id order by primary_flag desc) as rank
               from department)
               select employment_id,department_id from cte where rank=1
               """)


--------> dataframe way<--------------------

from pyspark.sql.window import Window
from pyspark.sql.functions import *
Window_fun=Window.partitionBy('employment_id').orderBy(col("primary_flag").desc())


df2=df.withColumn("row_number",dense_rank().over(Window_fun)).filter("row_number==1").select('employment_id','department_id')
df2.show()




