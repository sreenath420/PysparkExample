ğ—ªğ—µğ˜† ğ—¶ğ˜€ ğ—Ÿğ—®ğ˜‡ğ˜† ğ—²ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—œğ—ºğ—½ğ—¼ğ—¿ğ˜ğ—®ğ—»ğ˜ ğ—¶ğ—» ğ˜€ğ—½ğ—®ğ—¿ğ—¸ ?

Lazy evaluation is a key concept in Spark for optimizing and managing the execution of transformations and actions on distributed datasets. It defers the execution of transformations until an action is called, which allows Spark to optimize the execution plan and improve performance by minimizing unnecessary computations.

ğ—ªğ—¶ğ˜ğ—µğ—¼ğ˜‚ğ˜ ğ—Ÿğ—®ğ˜‡ğ˜† ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—»:
Every transformation (like map and filter) would be immediately executed.
If the dataset is huge, unnecessary computation could happen upfront, consuming a lot of resources and time.
This can lead to performance issues and even out-of-memory errors for large datasets.

ğ—ªğ—¶ğ˜ğ—µ ğ—Ÿğ—®ğ˜‡ğ˜† ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—»:
Spark doesn't execute any transformation until an action (like sum) is called.
It builds up a directed acyclic graph (DAG) of transformations, optimizing them along the way.
When an action is invoked, Spark analyzes the DAG and optimizes the execution plan before executing it.
This approach minimizes unnecessary computation and resource usage, making Spark more efficient, especially for big data processing.


data=[1,2,3,4,5]
rdd=sc.parallelize(data)
filter_rdd=rdd.filter(lambda x:x%2==0)
squred_rdd=filter_rdd.map(lambda x:x**2)
result=squred_rdd.reduce(lambda x,y:x+y)
print("Result",result)